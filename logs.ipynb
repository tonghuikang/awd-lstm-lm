{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attrs==19.1.0',\n",
       " 'backcall==0.1.0',\n",
       " 'bleach==3.1.0',\n",
       " 'decorator==4.4.0',\n",
       " 'defusedxml==0.6.0',\n",
       " 'entrypoints==0.3',\n",
       " 'ipykernel==5.1.1',\n",
       " 'ipython-genutils==0.2.0',\n",
       " 'ipython==7.5.0',\n",
       " 'ipywidgets==7.4.2',\n",
       " 'jedi==0.13.3',\n",
       " 'jinja2==2.10.1',\n",
       " 'jsonschema==3.0.1',\n",
       " 'jupyter-client==5.2.4',\n",
       " 'jupyter-console==6.0.0',\n",
       " 'jupyter-core==4.4.0',\n",
       " 'jupyter==1.0.0',\n",
       " 'markupsafe==1.1.1',\n",
       " 'mistune==0.8.4',\n",
       " 'nbconvert==5.5.0',\n",
       " 'nbformat==4.4.0',\n",
       " 'notebook==5.7.8',\n",
       " 'numpy==1.16.4',\n",
       " 'pandas==0.24.2',\n",
       " 'pandocfilters==1.4.2',\n",
       " 'parso==0.4.0',\n",
       " 'pexpect==4.7.0',\n",
       " 'pickleshare==0.7.5',\n",
       " 'pillow==6.0.0',\n",
       " 'prometheus-client==0.6.0',\n",
       " 'prompt-toolkit==2.0.9',\n",
       " 'ptyprocess==0.6.0',\n",
       " 'pygments==2.4.2',\n",
       " 'pyrsistent==0.15.2',\n",
       " 'python-dateutil==2.8.0',\n",
       " 'pytz==2019.1',\n",
       " 'pyzmq==18.0.1',\n",
       " 'qtconsole==4.5.1',\n",
       " 'send2trash==1.5.0',\n",
       " 'setuptools==41.0.1',\n",
       " 'six==1.12.0',\n",
       " 'terminado==0.8.2',\n",
       " 'testpath==0.4.2',\n",
       " 'torch==1.1.0',\n",
       " 'torchvision==0.3.0',\n",
       " 'tornado==6.0.2',\n",
       " 'traitlets==4.3.2',\n",
       " 'wcwidth==0.1.7',\n",
       " 'webencodings==0.5.1',\n",
       " 'widgetsnbextension==3.4.2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "# https://stackoverflow.com/questions/739993/how-can-i-get-a-list-of-locally-installed-python-modules\n",
    "sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torch.cuda.device object at 0x7f2372a6e208>\n",
      "1\n",
      "Tesla V100-SXM2-16GB\n",
      "True\n",
      "7501\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing dataset...\n",
      "Applying weight drop of 0.2 to weight_hh_l0\n",
      "Applying weight drop of 0.2 to weight_hh_l0\n",
      "Applying weight drop of 0.2 to weight_hh_l0\n",
      "[WeightDrop(\n",
      "  (module): LSTM(400, 1840)\n",
      "), WeightDrop(\n",
      "  (module): LSTM(1840, 1840)\n",
      "), WeightDrop(\n",
      "  (module): LSTM(1840, 400)\n",
      ")]\n",
      "Using []\n",
      "Args: Namespace(alpha=0.0, batch_size=128, beta=0.0, bptt=200, clip=0.25, cuda=True, data='data/enwik8', dropout=0.4, dropoute=0.0, dropouth=0.1, dropouti=0.1, emsize=400, epochs=50, log_interval=200, lr=0.001, model='LSTM', nhid=1840, nlayers=3, nonmono=5, optimizer='adam', resume='', save='ENWIK8.pt', seed=1111, tied=True, wdecay=1.2e-06, wdrop=0.2, when=[25, 35])\n",
      "Model total parameters: 47268441\n",
      "/home/hkmac/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "| epoch   1 |   200/ 3039 batches | lr 0.00100 | ms/batch 645.44 | loss  3.13 | ppl    22.76 | bpc    4.509\n",
      "| epoch   1 |   400/ 3039 batches | lr 0.00100 | ms/batch 631.02 | loss  2.43 | ppl    11.34 | bpc    3.504\n",
      "| epoch   1 |   600/ 3039 batches | lr 0.00100 | ms/batch 635.78 | loss  2.17 | ppl     8.77 | bpc    3.133\n",
      "| epoch   1 |   800/ 3039 batches | lr 0.00100 | ms/batch 633.28 | loss  2.00 | ppl     7.40 | bpc    2.887\n",
      "| epoch   1 |  1000/ 3039 batches | lr 0.00100 | ms/batch 633.49 | loss  1.90 | ppl     6.68 | bpc    2.740\n",
      "| epoch   1 |  1200/ 3039 batches | lr 0.00100 | ms/batch 640.22 | loss  1.83 | ppl     6.24 | bpc    2.641\n",
      "| epoch   1 |  1400/ 3039 batches | lr 0.00100 | ms/batch 641.02 | loss  1.77 | ppl     5.89 | bpc    2.558\n",
      "| epoch   1 |  1600/ 3039 batches | lr 0.00100 | ms/batch 636.83 | loss  1.74 | ppl     5.69 | bpc    2.509\n",
      "| epoch   1 |  1800/ 3039 batches | lr 0.00100 | ms/batch 632.15 | loss  1.68 | ppl     5.37 | bpc    2.425\n",
      "| epoch   1 |  2000/ 3039 batches | lr 0.00100 | ms/batch 627.61 | loss  1.69 | ppl     5.41 | bpc    2.436\n",
      "| epoch   1 |  2200/ 3039 batches | lr 0.00100 | ms/batch 643.44 | loss  1.66 | ppl     5.27 | bpc    2.397\n",
      "| epoch   1 |  2400/ 3039 batches | lr 0.00100 | ms/batch 638.59 | loss  1.65 | ppl     5.23 | bpc    2.386\n",
      "| epoch   1 |  2600/ 3039 batches | lr 0.00100 | ms/batch 641.31 | loss  1.61 | ppl     5.02 | bpc    2.329\n",
      "| epoch   1 |  2800/ 3039 batches | lr 0.00100 | ms/batch 636.86 | loss  1.60 | ppl     4.96 | bpc    2.311\n",
      "| epoch   1 |  3000/ 3039 batches | lr 0.00100 | ms/batch 634.60 | loss  1.59 | ppl     4.92 | bpc    2.300\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 2127.86s | valid loss  1.54 | valid ppl     4.65 | valid bpc    2.219\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   2 |   200/ 3039 batches | lr 0.00100 | ms/batch 638.62 | loss  1.58 | ppl     4.85 | bpc    2.277\n"
     ]
    }
   ],
   "source": [
    "!python3 -u main.py --epochs 50 --nlayers 3 --emsize 400 --nhid 1840 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.4 --wdrop 0.2 --wdecay 1.2e-6 --bptt 200 --batch_size 128 --optimizer adam --lr 1e-3 --data data/enwik8 --save ENWIK8.pt --when 25 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset...\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "[WeightDrop(\n",
      "  (module): LSTM(200, 1000)\n",
      "), WeightDrop(\n",
      "  (module): LSTM(1000, 1000)\n",
      "), WeightDrop(\n",
      "  (module): LSTM(1000, 200)\n",
      ")]\n",
      "Using []\n",
      "Args: Namespace(alpha=0.0, batch_size=128, beta=0.0, bptt=150, clip=0.25, cuda=True, data='data/pennchar', dropout=0.1, dropoute=0.0, dropouth=0.25, dropouti=0.1, emsize=200, epochs=500, log_interval=200, lr=0.002, model='LSTM', nhid=1000, nlayers=3, nonmono=5, optimizer='adam', resume='', save='PTBC.pt', seed=1111, tied=True, wdecay=1.2e-06, wdrop=0.5, when=[300, 400])\n",
      "Model total parameters: 13787650\n",
      "/home/hkmac/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "| epoch   1 |   200/  261 batches | lr 0.00200 | ms/batch 159.46 | loss  2.47 | ppl    11.84 | bpc    3.566\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 49.22s | valid loss  1.81 | valid ppl     6.10 | valid bpc    2.610\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   2 |   200/  261 batches | lr 0.00200 | ms/batch 157.20 | loss  1.67 | ppl     5.31 | bpc    2.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 49.51s | valid loss  1.36 | valid ppl     3.88 | valid bpc    1.956\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   3 |   200/  261 batches | lr 0.00200 | ms/batch 157.01 | loss  1.39 | ppl     4.00 | bpc    1.999\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 49.49s | valid loss  1.24 | valid ppl     3.46 | valid bpc    1.793\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   4 |   200/  261 batches | lr 0.00200 | ms/batch 158.76 | loss  1.29 | ppl     3.64 | bpc    1.865\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 49.22s | valid loss  1.19 | valid ppl     3.29 | valid bpc    1.719\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   5 |   200/  261 batches | lr 0.00200 | ms/batch 160.15 | loss  1.24 | ppl     3.47 | bpc    1.795\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 49.51s | valid loss  1.16 | valid ppl     3.18 | valid bpc    1.671\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   6 |   200/  261 batches | lr 0.00200 | ms/batch 158.86 | loss  1.21 | ppl     3.36 | bpc    1.748\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 49.46s | valid loss  1.14 | valid ppl     3.11 | valid bpc    1.639\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   7 |   200/  261 batches | lr 0.00200 | ms/batch 159.45 | loss  1.19 | ppl     3.28 | bpc    1.714\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 49.53s | valid loss  1.12 | valid ppl     3.06 | valid bpc    1.616\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   8 |   200/  261 batches | lr 0.00200 | ms/batch 158.21 | loss  1.17 | ppl     3.22 | bpc    1.687\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 49.42s | valid loss  1.11 | valid ppl     3.02 | valid bpc    1.597\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch   9 |   200/  261 batches | lr 0.00200 | ms/batch 161.16 | loss  1.15 | ppl     3.17 | bpc    1.666\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 49.61s | valid loss  1.10 | valid ppl     2.99 | valid bpc    1.580\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  10 |   200/  261 batches | lr 0.00200 | ms/batch 160.91 | loss  1.14 | ppl     3.13 | bpc    1.648\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 49.20s | valid loss  1.09 | valid ppl     2.96 | valid bpc    1.567\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  11 |   200/  261 batches | lr 0.00200 | ms/batch 156.48 | loss  1.13 | ppl     3.10 | bpc    1.634\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 49.59s | valid loss  1.08 | valid ppl     2.94 | valid bpc    1.557\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  12 |   200/  261 batches | lr 0.00200 | ms/batch 158.18 | loss  1.12 | ppl     3.08 | bpc    1.622\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 49.47s | valid loss  1.07 | valid ppl     2.92 | valid bpc    1.546\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  13 |   200/  261 batches | lr 0.00200 | ms/batch 158.63 | loss  1.11 | ppl     3.05 | bpc    1.608\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 49.32s | valid loss  1.07 | valid ppl     2.90 | valid bpc    1.538\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  14 |   200/  261 batches | lr 0.00200 | ms/batch 160.47 | loss  1.11 | ppl     3.03 | bpc    1.597\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 49.60s | valid loss  1.06 | valid ppl     2.89 | valid bpc    1.529\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  15 |   200/  261 batches | lr 0.00200 | ms/batch 159.30 | loss  1.10 | ppl     3.01 | bpc    1.588\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 49.45s | valid loss  1.06 | valid ppl     2.87 | valid bpc    1.522\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  16 |   200/  261 batches | lr 0.00200 | ms/batch 158.45 | loss  1.09 | ppl     2.99 | bpc    1.578\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 49.56s | valid loss  1.05 | valid ppl     2.86 | valid bpc    1.514\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  17 |   200/  261 batches | lr 0.00200 | ms/batch 157.30 | loss  1.09 | ppl     2.97 | bpc    1.570\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 49.59s | valid loss  1.04 | valid ppl     2.84 | valid bpc    1.507\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  18 |   200/  261 batches | lr 0.00200 | ms/batch 158.68 | loss  1.08 | ppl     2.95 | bpc    1.562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 49.52s | valid loss  1.04 | valid ppl     2.83 | valid bpc    1.502\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  19 |   200/  261 batches | lr 0.00200 | ms/batch 158.27 | loss  1.08 | ppl     2.94 | bpc    1.554\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 49.46s | valid loss  1.04 | valid ppl     2.82 | valid bpc    1.498\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  20 |   200/  261 batches | lr 0.00200 | ms/batch 160.60 | loss  1.07 | ppl     2.93 | bpc    1.549\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 49.14s | valid loss  1.03 | valid ppl     2.81 | valid bpc    1.492\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  21 |   200/  261 batches | lr 0.00200 | ms/batch 158.56 | loss  1.07 | ppl     2.91 | bpc    1.543\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 49.30s | valid loss  1.03 | valid ppl     2.81 | valid bpc    1.489\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  22 |   200/  261 batches | lr 0.00200 | ms/batch 160.03 | loss  1.07 | ppl     2.90 | bpc    1.537\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 49.15s | valid loss  1.03 | valid ppl     2.80 | valid bpc    1.484\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  23 |   200/  261 batches | lr 0.00200 | ms/batch 161.16 | loss  1.06 | ppl     2.89 | bpc    1.533\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 49.62s | valid loss  1.03 | valid ppl     2.79 | valid bpc    1.480\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  24 |   200/  261 batches | lr 0.00200 | ms/batch 159.20 | loss  1.06 | ppl     2.88 | bpc    1.527\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 49.54s | valid loss  1.02 | valid ppl     2.78 | valid bpc    1.477\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  25 |   200/  261 batches | lr 0.00200 | ms/batch 160.36 | loss  1.06 | ppl     2.88 | bpc    1.524\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 49.49s | valid loss  1.02 | valid ppl     2.77 | valid bpc    1.472\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  26 |   200/  261 batches | lr 0.00200 | ms/batch 161.67 | loss  1.05 | ppl     2.87 | bpc    1.519\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 49.16s | valid loss  1.02 | valid ppl     2.77 | valid bpc    1.470\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  27 |   200/  261 batches | lr 0.00200 | ms/batch 159.73 | loss  1.05 | ppl     2.86 | bpc    1.514\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 49.44s | valid loss  1.02 | valid ppl     2.76 | valid bpc    1.467\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  28 |   200/  261 batches | lr 0.00200 | ms/batch 160.31 | loss  1.05 | ppl     2.85 | bpc    1.511\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 49.47s | valid loss  1.01 | valid ppl     2.76 | valid bpc    1.463\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  29 |   200/  261 batches | lr 0.00200 | ms/batch 158.11 | loss  1.05 | ppl     2.84 | bpc    1.508\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 49.34s | valid loss  1.01 | valid ppl     2.75 | valid bpc    1.461\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  30 |   200/  261 batches | lr 0.00200 | ms/batch 159.74 | loss  1.04 | ppl     2.84 | bpc    1.504\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 49.54s | valid loss  1.01 | valid ppl     2.75 | valid bpc    1.458\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  31 |   200/  261 batches | lr 0.00200 | ms/batch 156.39 | loss  1.04 | ppl     2.83 | bpc    1.500\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 49.52s | valid loss  1.01 | valid ppl     2.74 | valid bpc    1.456\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  32 |   200/  261 batches | lr 0.00200 | ms/batch 159.94 | loss  1.04 | ppl     2.83 | bpc    1.498\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 49.39s | valid loss  1.01 | valid ppl     2.74 | valid bpc    1.453\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  33 |   200/  261 batches | lr 0.00200 | ms/batch 159.01 | loss  1.04 | ppl     2.82 | bpc    1.495\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 49.42s | valid loss  1.01 | valid ppl     2.73 | valid bpc    1.451\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  34 |   200/  261 batches | lr 0.00200 | ms/batch 158.91 | loss  1.03 | ppl     2.81 | bpc    1.491\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 49.40s | valid loss  1.00 | valid ppl     2.73 | valid bpc    1.450\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  35 |   200/  261 batches | lr 0.00200 | ms/batch 158.10 | loss  1.03 | ppl     2.81 | bpc    1.489\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 49.19s | valid loss  1.00 | valid ppl     2.73 | valid bpc    1.448\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  36 |   200/  261 batches | lr 0.00200 | ms/batch 159.06 | loss  1.03 | ppl     2.80 | bpc    1.486\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 49.14s | valid loss  1.00 | valid ppl     2.72 | valid bpc    1.446\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving model (new best validation)\n",
      "| epoch  37 |   200/  261 batches | lr 0.00200 | ms/batch 157.88 | loss  1.03 | ppl     2.80 | bpc    1.483\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 49.53s | valid loss  1.00 | valid ppl     2.72 | valid bpc    1.444\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model (new best validation)\n",
      "^C\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "!python3 -u main.py --epochs 500 --nlayers 3 --emsize 200 --nhid 1000 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.25 --dropouti 0.1 --dropout 0.1 --wdrop 0.5 --wdecay 1.2e-6 --bptt 150 --batch_size 128 --optimizer adam --lr 2e-3 --data data/pennchar --save PTBC.pt --when 300 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing WeightDrop\n",
      "=-=-=-=-=-=-=-=-=-=\n",
      "Testing WeightDrop with Linear\n",
      "Applying weight drop of 0.9 to weight\n",
      "All items should be different\n",
      "Run 1: [tensor(1.2150, device='cuda:0'), tensor(11.1652, device='cuda:0')]\n",
      "Run 2: [tensor(8.8544, device='cuda:0'), tensor(-7.6523, device='cuda:0')]\n",
      "---\n",
      "Testing WeightDrop with LSTM\n",
      "Applying weight drop of 0.9 to weight_hh_l0\n",
      "/home/hkmac/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "First timesteps should be equal, all others should differ\n",
      "Run 1: [tensor(0.3290, device='cuda:0'), tensor(-0.0117, device='cuda:0')]\n",
      "Run 2: [tensor(0.3290, device='cuda:0'), tensor(0.0147, device='cuda:0')]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "!python3 weight_drop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
